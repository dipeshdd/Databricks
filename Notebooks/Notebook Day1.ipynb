{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0858ecd5-1f5d-4c08-8494-d150b52b4d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark object is pre-provided. \n",
    "#spark.version shows Spark runtime.\n",
    "print(\"Hello Databricks from\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fedac02-cecc-42d1-b3c9-c76ca0fa4c57",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":208},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763112522100}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":228},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763112571767}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs is the Databricks File System utility\n",
    "#It helps you interact with files stored in:\n",
    "    #DBFS (Databricks File System), Azure Data Lake (ADLS) or S3 buckets mounted in DBFS, Local cluster storage paths, etc.\n",
    "#Think of it like Unix ls or Windows Explorer, but inside the Databricks workspace.\n",
    "\n",
    "#dbutils.fs.ls(path) -> lists all files and directories inside the given path.\n",
    "\n",
    "#display() is a Databricks notebook function that renders structured output in a rich table rather than plain text.\n",
    "\n",
    "#In Databricks, / (root) is the root of DBFS (Databricks File System).\n",
    "    #It is NOT the root of the cluster's local Linux filesystem.\n",
    "    #DBFS is a virtual filesystem built on top of cloud storage.\n",
    "\n",
    "display(dbutils.fs.ls(\"/\"))\n",
    "display(dbutils.fs.ls(\"/user/\")) #User-specific workspace directory\n",
    "\n",
    "#The output table helps you understand the directory structure inside DBFS, where notebooks, datasets, mounts, and uploaded files live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e6a14b-8739-4e6a-84d4-398fc3ccbbc2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":360},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763093592782}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/databricks-datasets/\"))\n",
    "#This is a special directory in DBFS that contains sample datasets provided by Databricks (e.g., airline data, retail data, public datasets).\n",
    "    #It’s often used for learning, demos, and experimentation.\n",
    "\n",
    "\n",
    "display(dbutils.fs.ls(\"/databricks-datasets/airlines/\"))\n",
    "#This lists the contents of the directory under /databricks-datasets/airlines/, which is a sample folder provided by Databricks.\n",
    "\n",
    "#What are the parts that we can see in the output of this command??\n",
    "    #Part Files: In distributed computing environments like Databricks, files are often split into multiple smaller partitioned files (or part files) when writing large datasets. Each of these smaller files contains a subset of the data.\n",
    "    #part-00000 is a subset of the entire airline data stored in a distributed fashion across multiple files. Each part contains a portion of the data, which, when combined, represents the full dataset.\n",
    "\n",
    "#Where Does This Come From?\n",
    "#Spark Write Operations: When Spark writes out a dataset to a distributed storage system (like DBFS), it often breaks the dataset into several part files based on the number of partitions in the data. These files are typically named part-00000, part-00001, etc., and you would find them in a directory where the data was saved.\n",
    "#Example Use Case: You might see part files in the /databricks-datasets/airlines/ directory if someone used Spark to process and save a large airline dataset in a partitioned manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bd8c1d-c92f-428e-8d24-8d0e02e25738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#What is a df (dataframe)??????????????\n",
    "    #A DataFrame in Spark is a distributed collection of data organized into named columns.\n",
    "    #It is similar to a table in a relational database or a data frame in Pandas (for Python).\n",
    "    #DataFrames in Spark are immutable (you can't modify them directly) \n",
    "        #and distributed, meaning that they are partitioned across multiple nodes in a Spark cluster for parallel processing.\n",
    "\n",
    "\n",
    "#Creating a DataFrame\n",
    "    #You can create a DataFrame in multiple ways, such as reading data from external files (CSV, Parquet, etc.) or from a Spark SQL query.\n",
    "\n",
    "#Reading a CSV file:\n",
    "    #Read data from an existing dataset into a DataFrame\n",
    "my_first_df = spark.read.csv(\"dbfs:/databricks-datasets/airlines/part-00000\", header=True, inferSchema=True) \n",
    "\n",
    "#Always Verify the path or use an existing dataset.\n",
    "    #read (DataFrameReader): Provides methods to load data from various formats (CSV, JSON, Parquet, JDBC, etc.)\n",
    "    #header=True (column names): Interprets the first row as column headers, not data. If False, Spark assigns default column names like _c0, _c1, etc.\n",
    "    #inferSchema=True (type detection): Spark scans the data to guess each column’s type (e.g., integer, double, timestamp).\n",
    "        #Without this, all columns are read as strings.\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "my_first_df.show(5)\n",
    "\n",
    "\n",
    "#-------If we want to try and load the entire dataset instead of just one subpart - Takes a lot of time due to HUGE amount of data-----#\n",
    "#   my_first_df1 = spark.read.csv(\"dbfs:/databricks-datasets/airlines/\", header=True, inferSchema=True)\n",
    "#   my_first_df1.show(5)\n",
    "\n",
    "\n",
    "my_first_df.printSchema()  # Displays the structure of the DataFrame\n",
    "\n",
    "my_first_df.columns  # List of column names\n",
    "\n",
    "my_first_df.describe().show()  # Summary statistics (like count, mean, stddev, min, max)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Hope you understood the flow : File system → file → DataFrame → display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46cd07a9-a196-41d5-b029-0c3e3ddb955c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select a single column\n",
    "my_first_df.select(\"Year\").show()\n",
    "\n",
    "#Showcase distinct values of a particular column\n",
    "my_first_df.select(\"ArrTime\").distinct().show()\n",
    "\n",
    "# Select multiple columns\n",
    "my_first_df.select(\"Year\", \"Origin\").show()\n",
    "\n",
    "# Select and alias a column\n",
    "my_first_df.select(my_first_df[\"Dest\"].alias(\"Destination\")).show()\n",
    "\n",
    "#The state of df persists across cells because it's kept in the same session’s memory.\n",
    "    #If you restart your cluster or session, though, that state is lost, and you'd need to re-run the cell that initializes the DataFrame.\n",
    "    #As long as the session is live, any variable (including DataFrames) you create in one cell will be available to other cells without needing to re-run or redefine them.\n",
    "\n",
    "\n",
    "#FILTERING DATA......\n",
    "# Filter rows based on a condition\n",
    "my_first_df.filter(my_first_df[\"DayofMonth\"] > 10).show()\n",
    "\n",
    "# You can also use SQL expressions for filtering\n",
    "my_first_df.filter(\"DayofMonth > 10\").show()\n",
    "\n",
    "\n",
    "\n",
    "#SORTING DATA.......\n",
    "    #You can sort the DataFrame based on one or more columns.\n",
    "\n",
    "# Sort in ascending order (default)\n",
    "my_first_df.orderBy(\"DayofMonth\").show()\n",
    "\n",
    "# Sort in descending order\n",
    "my_first_df.orderBy(my_first_df[\"DayofMonth\"].desc()).show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "my_first_df.orderBy(\"DayofMonth\", my_first_df[\"DayofWeek\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc6fe2a1-9c4f-4716-9128-12395ff0283b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook Day1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
